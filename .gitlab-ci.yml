# Attic Cache - GitLab CI/CD Pipeline
# ====================================
#
# Deploys Attic Nix binary cache to Kubernetes clusters.
# Configure cluster contexts and domains via CI/CD variables or organization.yaml.
#
# Authentication: Configurable (default: disabled for internal networks)
# Storage: MinIO (self-managed S3-compatible storage)
#
# Deployment Flow:
#   - Feature/MR branches: Review environment on dev cluster
#   - main branch: Auto-deploy to staging on prod cluster
#   - Semver tags (v*.*.*): Production deploy on prod cluster
#
# CI/CD Variables:
#   With MinIO (default): No S3 variables required
#   With External S3: S3_ENDPOINT, S3_ACCESS_KEY_ID, S3_SECRET_ACCESS_KEY, S3_BUCKET_NAME
#
# GitLab Kubernetes Agent:
#   Authorization via group: Configure in GitLab project settings
#   dev agent: $REVIEW_CLUSTER_CONTEXT
#   prod agent: $STAGING_CLUSTER_CONTEXT

workflow:
  auto_cancel:
    on_new_commit: interruptible
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: $CI_COMMIT_TAG =~ /^v[0-9]+\.[0-9]+\.[0-9]+/
    - if: $CI_PIPELINE_SOURCE == "schedule"

stages:
  - validate
  - build
  - test
  - deploy
  - verify

variables:
  # Namespace configuration
  NAMESPACE: "attic-cache"
  FIXED_SUBDOMAIN: "attic-cache"

  # Cluster contexts — set in CI/CD variables or organization.yaml
  # Review/dev cluster
  REVIEW_CLUSTER_CONTEXT: ""  # Set in CI/CD variables, e.g. "org/project/kubernetes/agents:dev"
  REVIEW_INGRESS_DOMAIN: ""   # Set in CI/CD variables, e.g. "dev.example.com"
  REVIEW_NAMESPACE: ${NAMESPACE}-review

  # Staging/production cluster
  STAGING_CLUSTER_CONTEXT: ""  # Set in CI/CD variables
  STAGING_INGRESS_DOMAIN: ""   # Set in CI/CD variables
  STAGING_NAMESPACE: ${NAMESPACE}-staging
  PROD_NAMESPACE: ${NAMESPACE}

  # Default context for pipeline (review environment)
  KUBE_CONTEXT: $REVIEW_CLUSTER_CONTEXT
  KUBE_INGRESS_BASE_DOMAIN: $REVIEW_INGRESS_DOMAIN

  # OpenTofu settings
  TF_IN_AUTOMATION: "true"
  TF_INPUT: "false"

  # Nix configuration
  NIX_CONFIG: |
    experimental-features = nix-command flakes
    accept-flake-config = true

  # Attic cache configuration (auth-free)
  ATTIC_SERVER: "https://${FIXED_SUBDOMAIN}.${KUBE_INGRESS_BASE_DOMAIN}"
  ATTIC_CACHE: "main"

  # Bazel remote cache (cluster-internal, set when bazel-cache is deployed)
  # Default endpoint assumes deployment with attic-cache-dev namespace
  BAZEL_REMOTE_CACHE: "grpc://bazel-cache.attic-cache-dev.svc.cluster.local:9092"

  # Disable unused AutoDevOps features
  TEST_DISABLED: "1"
  CODE_QUALITY_DISABLED: "1"
  POSTGRES_ENABLED: "false"

  # Kubernetes Agent warning suppress
  KUBERNETES_AGENT_WARNING_SUPPRESS: "true"

# Default job configuration
default:
  retry:
    max: 2
    when:
      - runner_system_failure
      - stuck_or_timeout_failure
  interruptible: true
  tags:
    - kubernetes

# Include templates and job definitions
include:
  # Security scanning
  - template: Jobs/SAST.gitlab-ci.yml
  - template: Jobs/Secret-Detection.gitlab-ci.yml
  # OpenTofu base templates (provides .tofu_with_k8s_tools)
  - local: ".gitlab/ci/templates/tofu-base.gitlab-ci.yml"
  # OpenTofu plan and apply jobs
  - local: ".gitlab/ci/jobs/tofu-plan.gitlab-ci.yml"
  - local: ".gitlab/ci/jobs/tofu-apply.gitlab-ci.yml"
  # Nix validation and build jobs
  - local: ".gitlab/ci/jobs/nix-check.gitlab-ci.yml"
  - local: ".gitlab/ci/jobs/nix-build.gitlab-ci.yml"
  # Bazel validation and build jobs (with remote cache)
  - local: ".gitlab/ci/jobs/bazel-build.gitlab-ci.yml"
  # Health check template
  - local: ".gitlab/ci/templates/verify-health.gitlab-ci.yml"
  # Runner smoke tests (manual trigger)
  - local: ".gitlab/ci/jobs/smoke-tests.gitlab-ci.yml"
  # Runner deployment pipeline (dev + prod clusters)
  - local: ".gitlab/ci/jobs/runners-deploy.gitlab-ci.yml"

# ==============================================================================
# Validate Stage - Configuration Validation
# ==============================================================================

validate:organization-config:
  stage: validate
  image: alpine:latest
  before_script:
    - apk add --no-cache yq bash
  script:
    - ./scripts/validate-org-config.sh config/organization.example.yaml
    - echo "✅ Organization config validation passed"
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH

# ==============================================================================
# Verify Stage - Health Checks
# ==============================================================================

# ==============================================================================
# Debug Job - Diagnose cluster state (manual trigger)
# ==============================================================================

debug:cluster:review:
  stage: verify
  extends: .tofu_with_k8s_tools
  when: manual
  allow_failure: true
  variables:
    KUBE_CONTEXT: $REVIEW_CLUSTER_CONTEXT
    DEBUG_NAMESPACE: "attic-cache-review"
  script:
    - |
      echo "=== Review Cluster Diagnostics ==="
      echo "Context: ${KUBE_CONTEXT}"
      echo "Namespace: ${DEBUG_NAMESPACE}"
      echo ""

      echo "=== Namespace Check ==="
      kubectl get namespace ${DEBUG_NAMESPACE} 2>/dev/null || echo "Namespace doesn't exist yet"

      echo ""
      echo "=== All Pods in Namespace ==="
      kubectl get pods -n ${DEBUG_NAMESPACE} -o wide 2>/dev/null || echo "No pods or namespace not found"

      echo ""
      echo "=== Pod Events ==="
      kubectl get events -n ${DEBUG_NAMESPACE} --sort-by='.lastTimestamp' 2>/dev/null | tail -30 || echo "No events"

      echo ""
      echo "=== Deployments ==="
      kubectl get deployments -n ${DEBUG_NAMESPACE} 2>/dev/null || echo "No deployments"

      echo ""
      echo "=== StatefulSets ==="
      kubectl get statefulsets -n ${DEBUG_NAMESPACE} 2>/dev/null || echo "No statefulsets"

      echo ""
      echo "=== Services ==="
      kubectl get services -n ${DEBUG_NAMESPACE} 2>/dev/null || echo "No services"

      echo ""
      echo "=== PVCs ==="
      kubectl get pvc -n ${DEBUG_NAMESPACE} 2>/dev/null || echo "No PVCs"

      echo ""
      echo "=== ConfigMaps ==="
      kubectl get configmaps -n ${DEBUG_NAMESPACE} 2>/dev/null || echo "No configmaps"

      echo ""
      echo "=== Secrets (names only) ==="
      kubectl get secrets -n ${DEBUG_NAMESPACE} 2>/dev/null || echo "No secrets"

      echo ""
      echo "=== Describe Failing Pods ==="
      for pod in $(kubectl get pods -n ${DEBUG_NAMESPACE} --field-selector=status.phase!=Running,status.phase!=Succeeded -o jsonpath='{.items[*].metadata.name}' 2>/dev/null); do
        echo "--- Pod: $pod ---"
        kubectl describe pod $pod -n ${DEBUG_NAMESPACE} | tail -50
        echo ""
      done

      echo ""
      echo "=== Check Operators ==="
      echo "MinIO Operator:"
      kubectl get pods -n minio-operator 2>/dev/null || echo "MinIO Operator namespace not found"
      echo ""
      echo "CNPG Operator:"
      kubectl get pods -n cnpg-system 2>/dev/null || echo "CNPG namespace not found"

# Auto-triggered debug on MR to capture cluster state
debug:auto:review:
  stage: verify
  extends: .tofu_with_k8s_tools
  needs:
    - job: tofu:apply:review
      optional: true
  allow_failure: true
  variables:
    KUBE_CONTEXT: $REVIEW_CLUSTER_CONTEXT
    DEBUG_NAMESPACE: "attic-cache-review"
  script:
    - |
      echo "=== Auto Debug: Review Cluster ==="
      echo "Namespace: ${DEBUG_NAMESPACE}"
      echo ""

      echo "=== Storage Classes ==="
      kubectl get sc || echo "No storage classes"

      echo ""
      echo "=== PVCs in Namespace ==="
      kubectl get pvc -n ${DEBUG_NAMESPACE} -o wide || echo "No PVCs"

      echo ""
      echo "=== Describe Pending PVCs ==="
      for pvc in $(kubectl get pvc -n ${DEBUG_NAMESPACE} -o jsonpath='{.items[?(@.status.phase!="Bound")].metadata.name}' 2>/dev/null); do
        echo "--- PVC: $pvc ---"
        kubectl describe pvc $pvc -n ${DEBUG_NAMESPACE} | tail -20
      done

      echo ""
      echo "=== Pods ==="
      kubectl get pods -n ${DEBUG_NAMESPACE} -o wide || echo "No pods"

      echo ""
      echo "=== Describe Pending/Failed Pods ==="
      for pod in $(kubectl get pods -n ${DEBUG_NAMESPACE} --field-selector=status.phase!=Running,status.phase!=Succeeded -o jsonpath='{.items[*].metadata.name}' 2>/dev/null); do
        echo "--- Pod: $pod ---"
        kubectl describe pod $pod -n ${DEBUG_NAMESPACE} 2>&1 | grep -A 30 "Events:" || kubectl describe pod $pod -n ${DEBUG_NAMESPACE} | tail -30
      done

      echo ""
      echo "=== Recent Events ==="
      kubectl get events -n ${DEBUG_NAMESPACE} --sort-by='.lastTimestamp' 2>/dev/null | tail -40 || echo "No events"

      echo ""
      echo "=== MinIO Tenant Status ==="
      kubectl get tenant -n ${DEBUG_NAMESPACE} -o wide 2>/dev/null || echo "No MinIO tenant"

      echo ""
      echo "=== CNPG Cluster Status ==="
      kubectl get cluster -n ${DEBUG_NAMESPACE} -o wide 2>/dev/null || echo "No CNPG cluster"

      echo ""
      echo "=== Container Logs (all non-ready pods) ==="
      # Get pods where containers are not ready (captures CrashLoopBackOff)
      # CrashLoopBackOff pods still have phase=Running but containers aren't ready
      for pod in $(kubectl get pods -n ${DEBUG_NAMESPACE} -o jsonpath='{range .items[*]}{.metadata.name}{" "}{range .status.containerStatuses[*]}{.ready}{" "}{end}{"\n"}{end}' 2>/dev/null | grep -E 'false' | awk '{print $1}'); do
        echo "--- Logs for pod: $pod ---"
        # Get status of containers
        echo "Container statuses:"
        kubectl get pod $pod -n ${DEBUG_NAMESPACE} -o jsonpath='{range .status.containerStatuses[*]}  {.name}: ready={.ready}, state={.state}{"\n"}{end}' 2>/dev/null || echo "  Unable to get status"
        echo ""
        # Get logs from all containers in the pod
        for container in $(kubectl get pod $pod -n ${DEBUG_NAMESPACE} -o jsonpath='{.spec.containers[*].name}' 2>/dev/null); do
          echo "Container: $container (current logs)"
          kubectl logs $pod -n ${DEBUG_NAMESPACE} -c $container --tail=30 2>&1 || echo "No logs available"
          echo ""
          echo "Container: $container (previous crash logs)"
          kubectl logs $pod -n ${DEBUG_NAMESPACE} -c $container --previous --tail=30 2>&1 || echo "No previous logs"
          echo ""
        done
        # Also check init containers
        for initcontainer in $(kubectl get pod $pod -n ${DEBUG_NAMESPACE} -o jsonpath='{.spec.initContainers[*].name}' 2>/dev/null); do
          echo "Init Container: $initcontainer"
          kubectl logs $pod -n ${DEBUG_NAMESPACE} -c $initcontainer --tail=20 2>&1 || echo "No logs available"
          echo ""
        done
      done
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"

verify:health:review:
  extends: .verify_health
  needs:
    - job: tofu:apply:review
      optional: true
  variables:
    # Review environments use dynamic hostname with branch slug
    HEALTH_URL: "https://${FIXED_SUBDOMAIN}-${CI_COMMIT_REF_SLUG}.${KUBE_INGRESS_BASE_DOMAIN}/"
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"

verify:health:staging:
  extends: .verify_health
  needs:
    - job: tofu:apply:staging
      optional: true
  variables:
    # Staging uses fixed hostname on rigel domain
    HEALTH_URL: "https://${FIXED_SUBDOMAIN}.${STAGING_INGRESS_DOMAIN}/"
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH

verify:health:production:
  extends: .verify_health
  needs:
    - job: tofu:apply:production
      optional: true
  variables:
    # Production uses fixed hostname on rigel domain
    HEALTH_URL: "https://${FIXED_SUBDOMAIN}.${STAGING_INGRESS_DOMAIN}/"
    # Faster checks for production (already deployed, should be healthy)
    HEALTH_MAX_ATTEMPTS: "10"
    HEALTH_INITIAL_DELAY: "15"
    HEALTH_MAX_DELAY: "30"
  rules:
    - if: $CI_COMMIT_TAG =~ /^v[0-9]+\.[0-9]+\.[0-9]+/
